{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/eugenewickett/logistigate/blob/main/docs/logistigate_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistigate\n",
    "## Overview\n",
    "Generally speaking, the logistigate methods infer aberration likelihoods at entities within a two-echelon supply chain, only using testing data from sample points taken from entities of the lower echelon. It is assumed that products originate within the system at one entity of the upper echelon, and are procured by one entity of the lower echelon. The likelihood of a lower-echelon entity obtaining product from each of the upper-echelon entities is stored in what is deemed the \"transition matrix\" for that system. Testing of products at the lower echelon yields aberrational (recorded as \"1\") or acceptable (\"0\") results. In what we deem the \"Tracked\" case, both the upper-echelon and lower-echelon entities traversed by the tested product are known upon testing. In the \"Untracked\" case, only the lower-echelon entity is entirely known, in addition to the system's transition matrix. It is further assumed that products are aberrational at their origin in the upper echelon with some fixed probability, and that products acceptable at the upper echelon become aberrational at the destination in the lower echelon with some other fixed probabiltiy. It is these fixed probabilities that the logistigate methods attempt to infer.\n",
    "\n",
    "More specifically, the logistigate methods were developed with the intent of inferring sources of substandard or falsified products within a pharmaceutical supply chain. Entities of the upper echelon are referred to as \"importers,\" and entities of the lower echelon are referred to as \"outlets.\" This terminology is used interchangeably throughout the logistigate package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Before using logistigate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make NumPy module available for numerical computations, as well as scipy and scipy.linalg\n",
    "# for linear algebra computations\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize options for Newton's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimizeObjInitOptions():\n",
    "    '''\n",
    "    Option initialization for minimize_obj function\n",
    "\n",
    "    Initialize algorithm options with default values\n",
    "\n",
    "    Return values:\n",
    "        options:\n",
    "            This is a dictionary with fields that correspond to algorithmic\n",
    "            options of our method.  In particular:\n",
    "\n",
    "            max_iter:\n",
    "                Maximum number of iterations\n",
    "            tol:\n",
    "                Convergence tolerance\n",
    "            step_type:\n",
    "                Different ways to calculate the search direction:\n",
    "               'gradient_descent'\n",
    "               'Newton'\n",
    "               'mod_Newton\n",
    "            init_alpha:\n",
    "                first trial step size to attempt during line search\n",
    "            suff_decrease_c1:\n",
    "                coefficient in sufficient decrease condition\n",
    "            output_level:\n",
    "                Amount of output printed\n",
    "                0: No output\n",
    "                1: Only summary information\n",
    "                2: One line per iteration (good for debugging)\n",
    "            rho_backtrack:\n",
    "                rho parameter for the backtracking algorithm\n",
    "            c_backtrack:\n",
    "                c parameter for the backtracking algorithm\n",
    "            mod_Newton_beta\n",
    "                beta parameter for Algorithm 3.3 of modified Newtons\n",
    "    '''\n",
    "    # we first need to create an empty dictionary before we can add entries\n",
    "    options = {}\n",
    "\n",
    "    # Now set a default value for each of the options\n",
    "    options['max_iter'] = 1e6\n",
    "    options['tol'] = 1e-6\n",
    "    options['step_type'] = 'Newton'\n",
    "    options['init_alpha'] = 1.\n",
    "    options['suff_decrease_c1'] = 1e-4\n",
    "    options['output_level'] = 2\n",
    "    options['rho_backtrack'] = 0.5\n",
    "    options['c_backtrack'] = 1e-4\n",
    "    options['mod_Newton_beta'] = 1e-4\n",
    "\n",
    "    return options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenbrock objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rosenbrock:\n",
    "    '''\n",
    "    Implementation of the Rosenbrock objective function\n",
    "\n",
    "    a*(x[1] - x[0]**2)**2 + (b - x[0])**2\n",
    "\n",
    "    Parameters:\n",
    "        a: scalar, see formula\n",
    "        b: scalar, see formula\n",
    "    '''\n",
    "\n",
    "    def __init__(self, a=100.0, b=1.0):\n",
    "        '''\n",
    "        Initialize object with parameters a and b in the formula.\n",
    "        '''\n",
    "        self._a = a\n",
    "        self._b = b\n",
    "\n",
    "    def value(self, x):\n",
    "        '''\n",
    "        Compute value of objective function at point x\n",
    "        '''\n",
    "        a = self._a\n",
    "        b = self._b\n",
    "        val = a*(x[1] - x[0]**2)**2 + (b - x[0])**2\n",
    "        return val\n",
    "\n",
    "    def gradient(self, x):\n",
    "        '''\n",
    "        Compute gradient of objective function at point x\n",
    "        '''\n",
    "        a = self._a\n",
    "        b = self._b\n",
    "        '''\n",
    "        Compute the gradient in terms of paramters a and b\n",
    "        '''\n",
    "        grad = np.array([(-4*a*x[0]*(x[1] - x[0]**2)) - (2*(b - x[0])), 2*a*(x[1]-x[0]**2)])\n",
    "        return grad\n",
    "\n",
    "    def hessian(self, x):\n",
    "        '''\n",
    "        Compute the Hessian of the objective function at point x\n",
    "        '''\n",
    "        a = self._a\n",
    "        Hess = np.array([[-4*a*x[1] + 12*a*(x[0]**2) + 2, -4*a*x[0]], [-4*a*x[0], 2*a]])\n",
    "        return Hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st exponential-based objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpoFunc1:\n",
    "    '''\n",
    "    Implementation of the 1st exponential-based objective function\n",
    "    '''\n",
    "\n",
    "    def __init__(self, a=1., b=1.):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        self._a = a\n",
    "        self._b = b\n",
    "\n",
    "    def value(self, x):\n",
    "        '''\n",
    "        Compute value of objective function at point x\n",
    "        '''\n",
    "        val = ((x[0]-1)**2)+((np.exp(x[1])-1)*((np.exp(x[1])+1)**(-1))) + 0.1*np.exp(-1*x[1])\n",
    "        return val\n",
    "\n",
    "    def gradient(self, x):\n",
    "        '''\n",
    "        Compute gradient of objective function at point x\n",
    "        '''\n",
    "        grad = np.array([2*(x[0]-1), -0.1*np.exp(-1*x[1]) + 2*np.exp(x[1])*((np.exp(x[1])+1)**(-2))])\n",
    "        return grad\n",
    "\n",
    "    def hessian(self, x):\n",
    "        '''\n",
    "        Compute Hessian of objective function at point x\n",
    "        '''\n",
    "        Hess = np.array([[2, 0], [0, 0.1*np.exp(-1*x[1]) - (2*np.exp(x[1])*(np.exp(x[1])-1))*((np.exp(x[1])+1)**(-3))]])\n",
    "        return Hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd exponential-based objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpoFunc2:\n",
    "    '''\n",
    "    Implementation of the 2nd exponential-based objective function\n",
    "    '''\n",
    "\n",
    "    def __init__(self, a=1., b=1.):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        self._a = a\n",
    "        self._b = b\n",
    "\n",
    "    def value(self, x):\n",
    "        '''\n",
    "        Compute value of objective function at point x\n",
    "        '''\n",
    "        val = ((x[0]-1)**4)+((np.exp(x[1])-1)*((np.exp(x[1])+1)**(-1))) + 0.1*np.exp(-1*x[1])\n",
    "        return val\n",
    "\n",
    "    def gradient(self, x):\n",
    "        '''\n",
    "        Compute gradient of objective function at point x\n",
    "        '''\n",
    "        grad = np.array([4*((x[0]-1)**3), -0.1*np.exp(-1*x[1]) + 2*np.exp(x[1])*((np.exp(x[1])+1)**(-2))])\n",
    "        return grad\n",
    "\n",
    "    def hessian(self, x):\n",
    "        '''\n",
    "        Compute Hessian of objective function at point x\n",
    "        '''\n",
    "        Hess = np.array([[12*((x[0]-1)**2), 0], [0, 0.1*np.exp(-1*x[1]) - (2*np.exp(x[1])*(np.exp(x[1])-1))*((np.exp(x[1])+1)**(-3))]])\n",
    "        return Hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Rosenbrock objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtRosenbrock:\n",
    "    '''\n",
    "    Implementation of the extended Rosenbrock objective function\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "        a: scalar, see formula\n",
    "        b: scalar, see formula\n",
    "        n: integer, see formula\n",
    "    '''\n",
    "\n",
    "    def __init__(self, a=100.0, b=1.0, n=10):\n",
    "        '''\n",
    "        Initialize object with parameters a, b and n in the formula.\n",
    "        '''\n",
    "        self._a = a\n",
    "        self._b = b\n",
    "        self._n = n\n",
    "\n",
    "    def value(self, x):\n",
    "        '''\n",
    "        Compute value of objective function at point x\n",
    "        '''\n",
    "        a = self._a\n",
    "        b = self._b\n",
    "        n = self._n\n",
    "        \n",
    "        #Initialize val at 0\n",
    "        val = 0\n",
    "        #loop through and sum the components of the function\n",
    "        for i in range(0,n-1):\n",
    "            tempval = 100*(x[i+1] - x[i]**2)**2 + (b - x[i])**2\n",
    "            val += tempval\n",
    "        return val\n",
    "\n",
    "    def gradient(self, x):\n",
    "        '''\n",
    "        Compute gradient of objective function at point x\n",
    "        '''\n",
    "        a = self._a\n",
    "        b = self._b\n",
    "        n = self._n\n",
    "        '''\n",
    "        Compute the gradient in terms of paramters a, b and n\n",
    "        '''\n",
    "        \n",
    "        #Initialize with the first value in the gradient vector\n",
    "        grad = np.array([(-4*a*x[0]*(x[1] - x[0]**2)) - (2*(b - x[0]))])\n",
    "        #Append successive values to the end of the gradient vector\n",
    "        for i in range(1,n-1):\n",
    "            tempval = np.array((-4*a*x[i]*(x[i+1] - x[i]**2)) - (2*(b - x[i])))\n",
    "            grad = np.append(grad, tempval)\n",
    "        #Append the nth value\n",
    "        tempval = np.array(2*a*(x[n-1] - x[n-2]**2))\n",
    "        grad = np.append(grad, tempval)\n",
    "        return grad\n",
    "\n",
    "    def hessian(self, x):\n",
    "        '''\n",
    "        Compute Hessian of objective function at point x\n",
    "        '''\n",
    "        a = self._a\n",
    "        b = self._b\n",
    "        n = self._n\n",
    "        \n",
    "        #Initialize an nxn Hessian of 0s\n",
    "        Hess = np.array([[0]*n]*n)\n",
    "        #Add values where necessary, starting with diagonals\n",
    "        Hess[0,0] = -4*a*x[1] + 12*a*(x[0]**2) + 2\n",
    "        for i in range(1,n-1):\n",
    "            Hess[i,i] = -4*a*x[i+1] + 12*a*(x[i]**2) + 2 + 2*a\n",
    "        Hess[n-1,n-1] = 2*a    \n",
    "        #Add values for i,j entries of Hessian, which follow the same formula\n",
    "        for i in range(0,n-1):\n",
    "            for j in range(0,n):\n",
    "                if i == j-1:\n",
    "                    Hess[i,j] = -4*a*x[i]\n",
    "                    Hess[j,i] = -4*a*x[i]\n",
    "        \n",
    "        return Hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's algorithm, with gradient descent and modified Newton's algorithm allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimizeObjective(x_start, objFunc, options):\n",
    "    '''\n",
    "    Optimization method for unconstrained optimization\n",
    "\n",
    "    Input arguments:\n",
    "        x_start:\n",
    "            Starting point\n",
    "        objFunc:\n",
    "            Objective function object.  It must have the following methods,\n",
    "            where x is the point where the quantity should be evaluated.\n",
    "\n",
    "            val = value(x)\n",
    "                returns value of objective function at x\n",
    "            grad = gradient(x)\n",
    "                returns gradient of objective function at x\n",
    "            Hess = hessian(x)\n",
    "                return Hessian of objective function at x\n",
    "        options:\n",
    "            This is a dictionary with options for the algorithm.\n",
    "            For details see the minimizeObjInitOptions function.\n",
    "\n",
    "    Return values:\n",
    "        status:\n",
    "           Return code indicating reason for termination:\n",
    "            0:  Optimal solution found (convergence tolerance satisfied)\n",
    "           -1:  Maximum number of iterations exceeded\n",
    "           -2:  Search direction is not a descent direction\n",
    "          -99:  Unknown error (bug?)\n",
    "        x_sol:\n",
    "            Approximate critical point (or last iterate if there is a failure)\n",
    "        f_sol:\n",
    "            objective function value at x_sol\n",
    "        stats:\n",
    "            Dictionary with statistics for the run.  Its fields are\n",
    "            norm_grad      Norm of gradient at final iterate\n",
    "            num_iter       Number of iterations taken\n",
    "            num_func_evals Number of function evaluations needed\n",
    "    '''\n",
    "\n",
    "    # initialize iteration counter\n",
    "    iter_count = 0\n",
    "\n",
    "    # initialize return flag (set to -99 so that we do not accidentally\n",
    "    # declare success.)\n",
    "    status = -99\n",
    "    \n",
    "    # get parameters out of the options dictionary\n",
    "    max_iter = options['max_iter']\n",
    "    tol = options['tol']\n",
    "    step_type = options['step_type']\n",
    "    init_alpha = options['init_alpha']\n",
    "    output_level = options['output_level']\n",
    "    rho = options['rho_backtrack']\n",
    "    c_btrack = options['c_backtrack']\n",
    "    beta = options['mod_Newton_beta']\n",
    "\n",
    "    # initialize current iterate\n",
    "    #\n",
    "    # ****Important note:****\n",
    "    # we need to make a copy of the x_start array.  If we would\n",
    "    # just write \"x_k = x_start\", then the Python variables x_k and x_start\n",
    "    # would be different names for the same array, and then changing\n",
    "    # x_k would also change x_start.  Since we do not want to change what is\n",
    "    # stored as the starting point, we need to make a copy here.\n",
    "    x_k = np.copy(x_start)\n",
    "\n",
    "    # initialize current function value, gradient, and infinity norm of the\n",
    "    # gradient\n",
    "    f_k = objFunc.value(x_k)\n",
    "    grad_k = objFunc.gradient(x_k)\n",
    "    norm_grad_k = np.linalg.norm(grad_k, np.inf)\n",
    "\n",
    "    # initialize counter for function evaluations\n",
    "    num_func_evals = 1\n",
    "\n",
    "    # determine how many variables are in the problem\n",
    "    num_vars = len(x_start)\n",
    "\n",
    "    # initialize search direction and its length to zero\n",
    "    p_k = np.zeros(num_vars)\n",
    "    norm_pk = 0.0\n",
    "\n",
    "    # initialize step size to zero (this is only necessary so that we can print\n",
    "    # an output line before a meaningful step size has actually been computed.)\n",
    "    alpha_k = 0.0\n",
    "\n",
    "    # initialize the counter for the function evaluations needed in a\n",
    "    # particular iteration\n",
    "    num_func_it = 0\n",
    "\n",
    "    # Print header and zero-th iteration for output\n",
    "    if output_level >= 2:\n",
    "        # (This is just a fancy way to create a string.  The '%s' formatting\n",
    "        # makes it easy to align the header with the actual output.)\n",
    "        output_header = '%6s %23s %9s %9s %6s %9s' % \\\n",
    "            ('iter', 'f', '||p_k||', 'alpha', '#func', '||grad_f||')\n",
    "        print(output_header)\n",
    "        print('%6i %23.16e %9.2e %9.2e %6i %9.2e' %\n",
    "              (iter_count, f_k, norm_pk, alpha_k, num_func_it, norm_grad_k))\n",
    "\n",
    "    ###########################\n",
    "    # Beginning of main Loop\n",
    "    ###########################\n",
    "\n",
    "    # We continue the main loop until the termination tolerance is met.\n",
    "    while 1:\n",
    "\n",
    "        ##############################################################\n",
    "        # Check termination tests\n",
    "        ##############################################################\n",
    "        if norm_grad_k <= tol:\n",
    "            # Termination tolerance met\n",
    "            status = 0\n",
    "            break\n",
    "\n",
    "        if iter_count >= max_iter:\n",
    "            # Set flag to indicate the maximum number of iterations has been\n",
    "            # exceeded\n",
    "            status = -1\n",
    "            # The following command says that we now want to leave the current\n",
    "            # loop (which is the while loop here).  The program execution will\n",
    "            # resume immediately after the end of the loop\n",
    "            break\n",
    "\n",
    "        ##############################################################\n",
    "        # Compute search direction\n",
    "        ##############################################################\n",
    "        #Netwons algorithm\n",
    "        if step_type == 'Newton':\n",
    "            #Retrieve the Hessian at the current x\n",
    "            hess_k = objFunc.hessian(x_k)\n",
    "            #We need to calculate the inverse\n",
    "            #...without calculating the ACTUAL inverse - very important!\n",
    "            #Perform Cholesky decomposition to obtain p_k\n",
    "            L = scipy.linalg.cholesky(hess_k, lower=True)\n",
    "            L_star = np.transpose(L)\n",
    "            y_chol = np.linalg.solve(L,-1*grad_k)\n",
    "            p_k = np.linalg.solve(L_star,y_chol)\n",
    "        \n",
    "        #gradient descent algorithm\n",
    "        elif step_type == 'gradient_descent':\n",
    "            p_k = (-1)*grad_k\n",
    "        \n",
    "        #modified Newtons algorithm\n",
    "        elif step_type == 'mod_Newton':\n",
    "            #initialize tau\n",
    "            tau_k = 0\n",
    "            #check the diagonals of the hessian and adjust tau accordingly\n",
    "            hess_k = objFunc.hessian(x_k)\n",
    "            if min(np.diagonal(hess_k)) < 0:\n",
    "                tau_k = -1*min(np.diagonal(hess_k)) + beta\n",
    "            #attempt to perfrom the Cholesky decomposition\n",
    "            L_boolean = False\n",
    "            while L_boolean == False:\n",
    "                try:\n",
    "                    #If the Hessian was already positive definite, tau would be zero, L will compute,\n",
    "                    # and all this error-handling will be unnecessary\n",
    "                    L = scipy.linalg.cholesky(hess_k + np.identity(len(hess_k))*tau_k, lower=True)\n",
    "                    L_boolean = True\n",
    "                except np.linalg.LinAlgError:\n",
    "                    L_boolean = False\n",
    "                    tau_k = max(2*tau_k,beta)\n",
    "                    pass\n",
    "            #Now that we've found an acceptable L, we can proceed with the Cholesky decomposition\n",
    "            L_star = np.transpose(L)\n",
    "            y_chol = np.linalg.solve(L,-1*grad_k)\n",
    "            p_k = np.linalg.solve(L_star,y_chol)\n",
    "        else:\n",
    "            raise ValueError('Invalid value for options[step_type]')\n",
    "\n",
    "        ##############################################################\n",
    "        # Perform the backtracking Armijo line search\n",
    "        ##############################################################\n",
    "\n",
    "        # initialize step size\n",
    "        alpha_k = init_alpha\n",
    "        \n",
    "        # Compute trial point and objective value at trial point\n",
    "        x_trial = x_k + alpha_k*p_k\n",
    "        f_trial = objFunc.value(x_trial)\n",
    "        num_func_it = num_func_it + 1\n",
    "        #Establish while condition and loop until Wolfe condition satisfied\n",
    "        #Backtracking parameters are pulled from the options dictionary\n",
    "        while f_trial > f_k + c_btrack*alpha_k*np.dot(grad_k, p_k):\n",
    "            alpha_k = rho*alpha_k\n",
    "            #Recompute the trial point and obj value\n",
    "            x_trial = x_k + alpha_k*p_k\n",
    "            f_trial = objFunc.value(x_trial)\n",
    "            #Increase the iterate count\n",
    "            num_func_it = num_func_it + 1\n",
    "\n",
    "        # Update iterate\n",
    "        x_k = np.copy(x_trial)\n",
    "        f_k = np.copy(f_trial)\n",
    "\n",
    "        # Compute gradient and its norm at the new iterate\n",
    "        grad_k = objFunc.gradient(x_k)\n",
    "        norm_grad_k = np.linalg.norm(grad_k, np.inf)\n",
    "\n",
    "        # For the output, compute the norm of the step\n",
    "        norm_pk = np.linalg.norm(p_k, np.inf)\n",
    "\n",
    "        # Update counter for total number of function evaluations\n",
    "        num_func_evals += num_func_it\n",
    "\n",
    "        # Increase the iteration counter\n",
    "        iter_count += 1\n",
    "\n",
    "        # Iteration output\n",
    "        if output_level >= 2:\n",
    "            # Print the output header every 10 iterations\n",
    "            if iter_count % 10 == 0:\n",
    "                print(output_header)\n",
    "            print('%6i %23.16e %9.2e %9.2e %6i %9.2e' %\n",
    "                  (iter_count, f_k, norm_pk, alpha_k, num_func_it, norm_grad_k))\n",
    "\n",
    "    ###########################\n",
    "    # End of main loop\n",
    "    ###########################\n",
    "\n",
    "    ###########################\n",
    "    # Finalize results\n",
    "    ###########################\n",
    "\n",
    "    # Set last iterate as the one that is returned, together with its objective\n",
    "    # value\n",
    "    x_sol = x_k\n",
    "    f_sol = f_k\n",
    "\n",
    "    # Set the statistics\n",
    "    stats = {}\n",
    "    stats['num_iter'] = iter_count\n",
    "    stats['norm_grad'] = norm_grad_k\n",
    "    stats['num_func_evals'] = num_func_evals\n",
    "\n",
    "    # Final output message\n",
    "    if output_level >= 1:\n",
    "        print('')\n",
    "        print('Final objective.................: %g' % f_sol)\n",
    "        print('||grad|| at final point.........: %g' % norm_grad_k)\n",
    "        print('Number of iterations............: %d' % iter_count)\n",
    "        print('Number of function evaluations..: %d' % num_func_evals)\n",
    "        print('')\n",
    "        if status == 0:\n",
    "            print('Exit: Critical point found.')\n",
    "        elif status == -1:\n",
    "            print('Exit: Maximum number of iterations (%d) exceeded.' %\n",
    "                  iter_count)\n",
    "        elif status == -2:\n",
    "            print('Exit: Search direction is not a descent direction.')\n",
    "        elif status == -3:\n",
    "            print('Exit: Exit: Step size becomes zero.')\n",
    "        else:\n",
    "            print('ERROR: Unknown status value: %d\\n' % status)\n",
    "\n",
    "    # Return output arguments\n",
    "    return status, x_sol, f_sol, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Rosenbrock with x_0 = (1.2,1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minimizeObjInitOptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-76a10b413055>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimizeObjInitOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gradient_descent'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mobjFunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRosenbrock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'minimizeObjInitOptions' is not defined"
     ]
    }
   ],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'gradient_descent'\n",
    "x_start = np.array([1.2, 1.2])\n",
    "objFunc = Rosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 6.71775e-13\n",
    "#||grad|| at final point.........: 9.9721e-07\n",
    "#Number of iterations............: 12960\n",
    "#Number of function evaluations..: 832287653\n",
    "#Exit: Critical point found. \n",
    "#x_sol: array([1.00000082, 1.00000164])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'Newton'\n",
    "x_start = np.array([1.2, 1.2])\n",
    "objFunc = Rosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 1.08829e-25\n",
    "#||grad|| at final point.........: 1.28799e-11\n",
    "#Number of iterations............: 8\n",
    "#Number of function evaluations..: 44\n",
    "#Exit: Critical point found.\n",
    "#x_sol: array([1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'mod_Newton'\n",
    "x_start = np.array([1.2, 1.2])\n",
    "objFunc = Rosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 1.08829e-25\n",
    "#||grad|| at final point.........: 1.28799e-11\n",
    "#Number of iterations............: 8\n",
    "#Number of function evaluations..: 44\n",
    "#Exit: Critical point found.\n",
    "#x_sol: array([1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 1 Comments\n",
    "We observe that all three methods convered to a critical point solution of $[1,1]$. Additionally, the Newton methods behaved the same, implying no adjustments were applied to the Hessian in any of the modified Newton iterations.\n",
    "\n",
    "However, we notice a marked difference in speed and computational efficiency, with the the steepest descent algorithm requiring 12,960 iterations, and 832,287,653 function evaluations, to achieve what the Newton-based methods did in 8 iterations. This large gap in efficiency illustrates the advantage of utilizing information about the curvature of $f(x)$ in determining search directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Rosenbrock with x_0 = (-1.2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minimizeObjInitOptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-76a10b413055>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimizeObjInitOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gradient_descent'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mobjFunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRosenbrock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'minimizeObjInitOptions' is not defined"
     ]
    }
   ],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'gradient_descent'\n",
    "x_start = np.array([-1.2, 1.])\n",
    "objFunc = Rosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 6.92304e-13\n",
    "#||grad|| at final point.........: 9.92743e-07\n",
    "#Number of iterations............: 13680\n",
    "#Number of function evaluations..: 929351532\n",
    "#Exit: Critical point found.\n",
    "#x_sol: array([0.99999917, 0.99999833])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'Newton'\n",
    "x_start = np.array([-1.2, 1])\n",
    "objFunc = Rosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 3.74398e-21\n",
    "#||grad|| at final point.........: 3.7325e-10\n",
    "#Number of iterations............: 21\n",
    "#Number of function evaluations..: 344\n",
    "#Exit: Critical point found.\n",
    "#x_sol: array([1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'mod_Newton'\n",
    "x_start = np.array([-1.2, 1.])\n",
    "objFunc = Rosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 3.74398e-21\n",
    "#||grad|| at final point.........: 3.7325e-10\n",
    "#Number of iterations............: 21\n",
    "#Number of function evaluations..: 344\n",
    "#Exit: Critical point found.\n",
    "#x_sol: array([1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 2 Comments\n",
    "Since all we are changing from Experiment 1 is the initial search solution, it will be interesting to compare the methods' performances in Experiment 2 as compared with their corresponding performances in Experiment 1.\n",
    "\n",
    "First analyzing the performance of the steepest descent algorithm, we observe a modest increase in iterations from  $12,960$ to $13,680$ - only a 5.3% increase due to a small change in initial search solution. Looking at our Newton-based methods, however (which again mirrored each other, likely due to a lack of need for Hessian modifications), we see an uptick in iterations from $8$ to $21$, signifying a doubling in the number of iterates required to acceptably converge. The sensitivity of Newton's method to the initial search solution is readily apparent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: 1st exponential-based function with x_0 = (-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minimizeObjInitOptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-76a10b413055>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimizeObjInitOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gradient_descent'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mobjFunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRosenbrock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'minimizeObjInitOptions' is not defined"
     ]
    }
   ],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'gradient_descent'\n",
    "x_start = np.array([-1., 1.])\n",
    "objFunc = ExpoFunc1()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: -0.205573\n",
    "#||grad|| at final point.........: 8.90619e-07\n",
    "#Number of iterations............: 22\n",
    "#Number of function evaluations..: 268\n",
    "#Exit: Critical point found.\n",
    "#x_sol: array([ 1., -1.2447683])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'Newton'\n",
    "x_start = np.array([-1., 1])\n",
    "objFunc = ExpoFunc1()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#LinAlgError: 2-th leading minor of the array is not positive definite\n",
    "#objFunc.hessian(x_start)\n",
    "    #array([[ 2.        ,  0.        ],\n",
    "           #[ 0.        , -0.14492755]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'mod_Newton'\n",
    "x_start = np.array([-1., 1.])\n",
    "objFunc = ExpoFunc1()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: -0.205573\n",
    "#||grad|| at final point.........: 2.06291e-07\n",
    "#Number of iterations............: 5\n",
    "#Number of function evaluations..: 66\n",
    "#Exit: Critical point found.\n",
    "#x_sol: array([ 1., -1.24477034])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 3 Comments\n",
    "Running the steepest descent method with this new objective function that has a lot of different exponential terms, we see a relatively quick convergence in only $22$ iterations, implying that more sophisticated algorithms should have an even easier time of handling this problem. Our non-modified Newton's method, however, returns a linear algebra error, and upon further inspection we notice that the hessian of our objective function, evaluated at this initial point, is not positive definite, as determined by the negative value in the diagonal. This situation prevents the working ability of this method.\n",
    "\n",
    "Upon allowing Hessian modifications, however, we see that the modified Newton's method was able to converge to a solution in only $5$ iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4: 2nd exponential-based function with x_0 = (-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minimizeObjInitOptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-76a10b413055>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimizeObjInitOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gradient_descent'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mobjFunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRosenbrock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'minimizeObjInitOptions' is not defined"
     ]
    }
   ],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'gradient_descent'\n",
    "x_start = np.array([-1., 1.])\n",
    "objFunc = ExpoFunc2()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: -0.205573\n",
    "#||grad|| at final point.........: 6.65928e-07\n",
    "#Number of iterations............: 49\n",
    "#Number of function evaluations..: 4407\n",
    "#Exit: Critical point found.\n",
    "#x_sol: array([ 1., -1.24476872])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'Newton'\n",
    "x_start = np.array([-1., 1])\n",
    "objFunc = ExpoFunc2()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#LinAlgError: 2-th leading minor of the array is not positive definite\n",
    "#objFunc.hessian(x_start)\n",
    "#    array([[ 2.        ,  0.        ],\n",
    "#           [ 0.        , -0.14492755]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'mod_Newton'\n",
    "x_start = np.array([-1., 1.])\n",
    "objFunc = ExpoFunc2()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: -0.205573\n",
    "#||grad|| at final point.........: 3.80733e-07\n",
    "#Number of iterations............: 16\n",
    "#Number of function evaluations..: 297\n",
    "#Exit: Critical point found.\n",
    "#x_sol: array([ 0.99543417, -1.24476995])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 4 Comments\n",
    "The only change from Experiment 3 to Experiment 4 is a single quadratic term becoming a fourth-order term. Given the effect we would expect this type of change to have on the Hessian, it makes sense to see an uptick in the number of iterations required for the modified Newton's method to converge.\n",
    "\n",
    "What is more interesting, though, is the effect this change had on the steepest descent method - convergence required more than twice as many iterations. The descent method seemingly focused more on the steeper gradient generated by the fourth-order term, despite the fact that $x_1$ was only included in one term of the objective function.\n",
    "\n",
    "Collectively, these changes reflect the effect even small gradient changes can have on the ability of our line search methods to find efficient search directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5: extended Rosenbrock function with n=10, x_0 = (-1,...,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minimizeObjInitOptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-76a10b413055>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimizeObjInitOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gradient_descent'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mobjFunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRosenbrock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'minimizeObjInitOptions' is not defined"
     ]
    }
   ],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'gradient_descent'\n",
    "#Construct our initial x\n",
    "n = 10\n",
    "x_start = np.array([-1]*n)\n",
    "objFunc = ExtRosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 2.0601e-12\n",
    "#||grad|| at final point.........: 9.92607e-07\n",
    "#Number of iterations............: 14643\n",
    "#Number of function evaluations..: 1059596858\n",
    "#Exit: Critical point found.\n",
    "#x_sol: \n",
    "#array([0.99999999, 0.99999999, 0.99999998, 0.99999996, 0.99999992,\n",
    "#       0.99999985, 0.99999969, 0.99999938, 0.99999876, 0.99999751])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'Newton'\n",
    "options['max_iter'] = 1e2\n",
    "#Construct our initial x\n",
    "n = 10\n",
    "x_start = np.array([-1]*n)\n",
    "objFunc = ExtRosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 113.401\n",
    "#||grad|| at final point.........: 15.7087\n",
    "#Number of iterations............: 100\n",
    "#Number of function evaluations..: 138964\n",
    "#Exit: Maximum number of iterations (100) exceeded.\n",
    "#x_sol: \n",
    "#array([ 0.32115112,  0.09097437, -0.0172405 ,  0.08747292,  0.21818123,\n",
    "#        0.20968267,  0.0927851 ,  0.00639282, -0.97545577,  0.95113202])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'mod_Newton'\n",
    "options['max_iter'] = 1e2\n",
    "#Construct our initial x\n",
    "n = 10\n",
    "x_start = np.array([-1]*n)\n",
    "objFunc = ExtRosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 113.401\n",
    "#||grad|| at final point.........: 15.7087\n",
    "#Number of iterations............: 100\n",
    "#Number of function evaluations..: 138964\n",
    "#Exit: Maximum number of iterations (100) exceeded.\n",
    "#x_sol: \n",
    "#array([ 0.32115112,  0.09097437, -0.0172405 ,  0.08747292,  0.21818123,\n",
    "#        0.20968267,  0.0927851 ,  0.00639282, -0.97545577,  0.95113202])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 5 Comments\n",
    "Switching gears to the extended Rosebrock function, we first note that the performance of the steepest descent method is relatively unaffected in the switch from $2$ to $10$ dimensions - our iterations only increase from $12,960$ to $14643$, whereas from the infamous \"curse of dimensionality\" we would anticipate a much larger increase in the number of iterations required to reach convergence.\n",
    "\n",
    "But avoiding some of the hang-ups of the shape of the Rosenbrock function seems to be beneficial in this case: our Newton-based methods did not converge, and instead seemed to become trapped at a location with a positive gradient. Both methods mirrored each other, so the positive definite-ness of the Hessian does not seem to have been a problem. Given how the difference between the steepest descent method and the Newton methods rests on the Hessian, though, it would appear that either of the following may be true:\n",
    "\n",
    "$\\quad$ a) The shape of the Rosenbrock function in larger dimensions leads to an unwieldy Hessian that causes extremely small step sizes, and hence leaves us in areas with positive gradients. Looking at the individual iteration results, we do indeed see improvements in the overall objective function, but after about $30$ iterations, the improvements are on the order of $10^{-15}$, implying that the methods would perhaps indeed converge if give enough iterations (beyond the capacity of my laptop).\n",
    "\n",
    "$\\quad$ b) The calculation used for the Hessian in the class definition of ExtRosenbrock() is faulty or miscalculated (certainly a distinct possibility!).\n",
    "\n",
    "Of important consideration, too, is the final value of the objective function - see the comment below Experiment 8 for a discussion of this element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6: extended Rosenbrock function with n=10, x_0 = (2,...,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'gradient_descent'\n",
    "#Construct our initial x\n",
    "nvar = 10\n",
    "x_start = np.array([2]*nvar)\n",
    "objFunc = ExtRosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 2.02967e-12\n",
    "#||grad|| at final point.........: 9.95519e-07\n",
    "#Number of iterations............: 13919\n",
    "#Number of function evaluations..: 963405383\n",
    "#Exit: Critical point found.\n",
    "#x_sol: \n",
    "#array([1.        , 1.00000001, 1.00000002, 1.00000004, 1.00000008,\n",
    "#       1.00000015, 1.00000031, 1.00000062, 1.00000123, 1.00000247])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'Newton'\n",
    "options['max_iter'] = 1e2\n",
    "#Construct our initial x\n",
    "nvar = 10\n",
    "x_start = np.array([2]*nvar)\n",
    "objFunc = ExtRosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 1.68495\n",
    "#||grad|| at final point.........: 21.8343\n",
    "#Number of iterations............: 100\n",
    "#Number of function evaluations..: 245332\n",
    "#Exit: Maximum number of iterations (100) exceeded.\n",
    "#x_sol: \n",
    "#array([0.99721777, 0.96336326, 0.98454016, 0.97900323, 0.94710425,\n",
    "#       0.88145166, 0.76007831, 0.55273574, 0.25281819, 0.04990802])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'mod_Newton'\n",
    "options['max_iter'] = 1e2\n",
    "#Construct our initial x\n",
    "nvar = 10\n",
    "x_start = np.array([2]*nvar)\n",
    "objFunc = ExtRosenbrock()\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 1.68495\n",
    "#||grad|| at final point.........: 21.8343\n",
    "#Number of iterations............: 100\n",
    "#Number of function evaluations..: 245332\n",
    "#Exit: Maximum number of iterations (100) exceeded.\n",
    "#x_sol: \n",
    "#array([0.99721777, 0.96336326, 0.98454016, 0.97900323, 0.94710425,\n",
    "#       0.88145166, 0.76007831, 0.55273574, 0.25281819, 0.04990802])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 6 Comments\n",
    "The only change from Experiment 5 is the initial solution point, and we see a modest improvement in the steepest descent method upon this change ($13,919$ iterations, a 4.9% reduction from $14,642$ iterations). We still observe with our Newton methods, however, the same inertia in improvement after about $10$ iterations, with the discussion points from Experiment 5 remaining pertinent for Experiment 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7: extended Rosenbrock function with n=100, x_0 = (-1,...,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'gradient_descent'\n",
    "#Construct our initial x\n",
    "nvar = 100\n",
    "x_start = np.array([-1]*nvar)\n",
    "objFunc = ExtRosenbrock(a=100,b=1,n=nvar)\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 2.06718e-12\n",
    "#||grad|| at final point.........: 9.96223e-07\n",
    "#Number of iterations............: 15336\n",
    "#Number of function evaluations..: 1181346295\n",
    "#Exit: Critical point found.\n",
    "#x_sol: \n",
    "#array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
    "#       1.        , 0.99999999, 0.99999998, 0.99999996, 0.99999992,\n",
    "#       0.99999985, 0.99999969, 0.99999938, 0.99999876, 0.99999751])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'Newton'\n",
    "options['max_iter'] = 40\n",
    "#Construct our initial x\n",
    "nvar = 100\n",
    "x_start = np.array([-1]*nvar)\n",
    "objFunc = ExtRosenbrock(a=100,b=1,n=nvar)\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 251.867\n",
    "#||grad|| at final point.........: 10.1008\n",
    "#Number of iterations............: 40\n",
    "#Number of function evaluations..: 5290\n",
    "#Exit: Maximum number of iterations (40) exceeded.\n",
    "#x_sol: \n",
    "#array([ 0.67280312,  0.41269993,  0.11358598, -0.01547916,  0.02405063,\n",
    "#        0.03167903,  0.0320081 ,  0.03206548,  0.03206992,  0.03216049,\n",
    "#        0.03214279,  0.03212917,  0.03213202,  0.03213147,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#       0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213155,  0.03213152,  0.03213159,  0.03213139,  0.03213193,\n",
    "#       0.03213053,  0.0321339 ,  0.03212402,  0.03210004,  0.03206758,\n",
    "#       0.03208163,  0.03207614,  0.03221117,  0.03130586,  0.03292693,\n",
    "#        0.02853627,  0.03572528,  0.01629274,  0.03764733, -0.02195437,\n",
    "#        0.03496872, -0.05777477,  0.00341973, -1.21059804,  1.46532997])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'mod_Newton'\n",
    "options['max_iter'] = 40\n",
    "#Construct our initial x\n",
    "nvar = 100\n",
    "x_start = np.array([-1]*nvar)\n",
    "objFunc = ExtRosenbrock(a=100,b=1,n=nvar)\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 251.867\n",
    "#||grad|| at final point.........: 10.1008\n",
    "#Number of iterations............: 40\n",
    "#Number of function evaluations..: 5290\n",
    "#Exit: Maximum number of iterations (40) exceeded.\n",
    "#x_sol: \n",
    "#array([ 0.67280312,  0.41269993,  0.11358598, -0.01547916,  0.02405063,\n",
    "#        0.03167903,  0.0320081 ,  0.03206548,  0.03206992,  0.03216049,\n",
    "#        0.03214279,  0.03212917,  0.03213202,  0.03213147,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#       0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#       0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213154,  0.03213154,  0.03213154,  0.03213154,  0.03213154,\n",
    "#        0.03213155,  0.03213152,  0.03213159,  0.03213139,  0.03213193,\n",
    "#        0.03213053,  0.0321339 ,  0.03212402,  0.03210004,  0.03206758,\n",
    "#        0.03208163,  0.03207614,  0.03221117,  0.03130586,  0.03292693,\n",
    "#        0.02853627,  0.03572528,  0.01629274,  0.03764733, -0.02195437,\n",
    "#        0.03496872, -0.05777477,  0.00341973, -1.21059804,  1.46532997])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 7 Comments\n",
    "Again, the effect on the steepest descent method of increasing the dimensionality is surprisingly lower than one might anticipate, with our iterations only increasing from $13,919$ to $15,336$, a 10.2% increase after a 10-fold increase in dimensionality.\n",
    "\n",
    "Once again, however, the Newton methods got \"stuck\" after about 20 iterations, registering scant improvement afterwards. (See below comment on Experiment 8 regarding the overall objective value.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8: extended Rosenbrock function with n=10000, x_0 = (2,...,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'gradient_descent'\n",
    "options['max_iter'] = 3e3\n",
    "#Construct our initial x\n",
    "nvar = 10000\n",
    "x_start = np.array([2]*nvar)\n",
    "objFunc = ExtRosenbrock(a=100,b=1,n=nvar)\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 92.9406\n",
    "#||grad|| at final point.........: 1.95748\n",
    "#Number of iterations............: 3000\n",
    "#Number of function evaluations..: 211308995\n",
    "#Exit: Maximum number of iterations (3000) exceeded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'Newton'\n",
    "options['max_iter'] = 50\n",
    "#Construct our initial x\n",
    "nvar = 10000\n",
    "x_start = np.array([2]*nvar)\n",
    "objFunc = ExtRosenbrock(a=100,b=1,n=nvar)\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 2.54134\n",
    "#||grad|| at final point.........: 10.29\n",
    "#Number of iterations............: 50\n",
    "#Number of function evaluations..: 2289\n",
    "#Exit: Maximum number of iterations (50) exceeded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = minimizeObjInitOptions()\n",
    "options['step_type'] = 'mod_Newton'\n",
    "options['max_iter'] = 50\n",
    "#Construct our initial x\n",
    "nvar = 10000\n",
    "x_start = np.array([2]*nvar)\n",
    "objFunc = ExtRosenbrock(a=100,b=1,n=nvar)\n",
    "\n",
    "status, x_sol, f_sol, stats = minimizeObjective(x_start, objFunc, options)\n",
    "x_sol\n",
    "\n",
    "###\n",
    "#Final objective.................: 2.54134\n",
    "#||grad|| at final point.........: 10.29\n",
    "#Number of iterations............: 50\n",
    "#Number of function evaluations..: 2289\n",
    "#Exit: Maximum number of iterations (50) exceeded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 8 Comments\n",
    "Lastly, we attempt to minimize a 10,000-variable extended Rosenbrock function, for which iterations had to be capped for processing speed considerations ($3,000$ for steepest descent, $100$ for Newton-based). It is hard to say how many more iterations would be required for the steepest descent method to converge, but using an estimated 10% increase for each 10-fold increase in variables (from Experiment 7), we should expect the steepest descent method to converge after approximately $20,000$ iterations.\n",
    "\n",
    "The Newton methods became \"stuck\" after only 5 iterations in this experiment. And a very intriguing result of Experiment 8 is how much lower the final objective value is for our Newton-based methods than that of Experiment 7: $2.54$ versus $251.87$, and how similar it is to that of Experiment 6: $2.54$ versus $1.68$. This fact highlights how crucial initial starting points are for the effectiveness of the Newton methods; in the case of the extended Rosenbrock function, increasing the dimensionality from $100$ to $10,000$ had a minimal effect on the ultimate performance when the initial starting point was selected more beneficially."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
